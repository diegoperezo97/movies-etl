# MODULE 8: ETL - Extract, Transform, Load

I'll study about the Extract, Transform, and Load (ETL) data pipeline process in this subject. The mainstay for transferring data across databases is ETL. For instance, in order to increase performance, Google and Amazon frequently move data across different sites. ETL is a fundamental idea in data engineering that guarantees consistency and data integrity. Any type of analysis is impossible without reliable, consistent data. An effective ETL pipeline aims to automate as much data manipulation as possible. The time it takes to execute analyses can be decreased by using the ETL method to generate data stores that operate more effectively. Python, Pandas, and Postgres SQL will be used to accomplish our data wrangling, and Postgres SQL will be used to store our completed data.

Let's look in-depth at the ETL process.

## Overview of the Analysis
* Amazing Prime Video is the world's leading online retailer's streaming platform for movies and television shows. The Amazing Prime video team wants to create an algorithm that can anticipate which low-budget films will become popular and then buy the streaming rights for a low price. 
* Amazing Prime has decided to fund a hackathon in order to motivate the team, have some fun, and engage with the local coding community. Giving participants a clean data set of movie data and asking them to guess the most popular films. Britta, a member of the Amazing Prime video team, has been assigned the duty of creating the hackathon datasets.

### Purpose
* Help Britta  extract data from both sources, combine it into a single clean data set, and then import it into a SQL table.
